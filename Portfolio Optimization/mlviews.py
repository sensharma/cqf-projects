import numpy as np
import pandas as pd
import scipy
import datetime as dt

from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.naive_bayes import GaussianNB as NBC
from sklearn.svm import SVC

from portanalytics import *

ertn = pd.read_excel('02exc_returns.xlsx')
ret = pd.read_excel('01returns.xlsx')
exfeat = pd.read_excel('06ext-features.xlsx')

c_list = [RFC(), NBC(), SVC(probability=True)]

# No. of classes/ labels/ states (just used as a global)
classes = 5


def cLabel(rtn, avg, stdev):
    """
    Helper function to generate labels based on variation from mean
    :param rtn: the return column of pandas to which it is applied
    :param avg: the mean of the series
    :param stdev: the standard deviation of the series
    :return: labels -2, -1, 0, +1, +2; based on 0 +/- 0.5*stdev and 0 +/- 1*stdev
    """

    if abs(rtn-avg)<0.5*stdev:
        return 0
    elif abs(rtn-avg)<stdev:
        return np.sign(rtn)*1
    else:
        return np.sign(rtn)*2


def classify(clf, dframe, exfeat, todate):
    """
    Takes ER data, external features, concatenates them and classifies for all featrues provided,
    for the last date in the date range given
    :param clf: Classifier type. They HAVE to be function CALLS!! Like clf = NBC()
    :param dframe: The full excess return dataframe, BUT HAS TO BE WEEKLY RESAMPLED AT INPUT
    :param exfeat: External features list
    :param todate: Weekly date for which prediction reqd.
    :return: pred date, pred list, pred probs and pred scores
    """

    todate = todate if isinstance(todate, dt.datetime) else dt.datetime.strptime(str(todate), '%Y%m%d')
    clf_list = []
    pred_list = []
    prob_list = []
    score_list = []
    cols = dframe.columns

    # Preparing the data and aligning dates to run classification
    for col in cols:
        feat = pd.DataFrame()
        feat[col] = dframe[col]
        feat = pd.concat([feat, exfeat], axis=1, copy=True)
        feat = feat.resample('W').sum()
        dtd = todate
        feat['WExpRet'] = feat[col].shift(-1)
        train_feat = feat[[col, 'SPY', 'VIX', 'GLD', 'PCR', 'USDI', 'WExpRet']].ix[:todate, :]
        train_feat = train_feat.ix[:-1, :] if train_feat.index[-1] > dtd else train_feat
        feat.dropna(inplace=True)
        trn_mean = feat[col].mean()
        trn_stdev = feat[col].std()
        train_feat['Label'] = train_feat.WExpRet.apply(cLabel, avg=trn_mean, stdev=trn_stdev)
        clf_list.append(train_feat)

    # Training (fitting) the classifier and predicting for each asset
    for c, df in enumerate(clf_list):
        df_feat = df.iloc[:-1, :-2]
        df_labels = df.iloc[:-1, -1]
        clf = clf.fit(df_feat, df_labels)
        ind = df.index[-1]
        df_pred = df.iloc[:, :-2]
        cls = clf.predict(df_pred.ix[ind].reshape(1, -1))
        pred_list.append(cls)
        prob = clf.predict_proba(df_pred.ix[ind].reshape(1, -1)).flatten()
        prob_list.append(prob)
        score = clf.score(df_pred.ix[ind].reshape(1, -1), cls)
        score_list.append(score)
    return df_pred.index[-1], pred_list, prob_list, score_list


def getMlViews(preds, Pi, V, conf):
    """
    Calculating the views from the predictions generated by the ML classifiers, using the market volatility based
    confidence interval approach of Meucci. A lot of care needs to be taken to get rid of the 0 rows properly.
    :param preds: predictions list from classify()
    :param Pi: the BL prior
    :param V: the covariance matrix
    :param conf: the overall confidence in the views, from ML backtesting
    :return: pick matrix, views vector and views covariance omega
    """

    eta = np.array(preds)
    pick = np.sign(eta)
    pick = pd.DataFrame(np.diag(pick.flatten()))
    Sig = np.sqrt(np.dot(np.dot(pick, V), pick.T))
    Sig = pd.DataFrame(np.diag(Sig).reshape(len(Sig), 1))

    ## Rows with 0s being accounted for using useful Pandas trick
    Sig = Sig[(Sig.T != 0).any()]
    pPi = pd.DataFrame(np.dot(pick, Pi))
    pPi = pPi[(pPi.T != 0).any()]
    eta = pd.DataFrame(eta)
    eta = eta[(eta.T != 0).any()]
    v = pPi + eta*Sig
    v = pd.DataFrame(v)
    pick = pick[(pick.T != 0).any()]
    omega = (1/conf)*np.sqrt(np.dot(np.dot(pick, V), pick.T))
    omega = np.diag(np.diag(omega))
    omega = pd.DataFrame(omega)
    return pick, v, omega


def mlBacktest(clf, df, exfeat, startdate=None, todate=None):
    """
    Backtests the classifier on a weekly running basis, comparing predictions vs actual outputs
    :param clf: the classifier used
    :param df: full excess return dataframe
    :param exfeat: external features used in classification
    :param startdate: day from which to start the backtest
    :param todate: day till which backtest is to be carried out
    :return: output of the backtest as a pandas Panel (3D), with all predictions, probabilities & estimations of
    correctness
    """

    # Setting up the output - designed as a pandas data panel (3D)
    startdate = dt.datetime(2013, 12, 31) if startdate is None else startdate
    todate = dt.datetime(2016, 12, 31) if todate is None else todate
    dframe = df.resample('W').sum()
    dt_range = pd.date_range(start=startdate, end=todate, freq='W')
    cols = ['Pred', 'P1', 'P2', 'P3', 'P4', 'P5', 'Act', 'ActLabel', 'PrevMean', 'PrevSD',
            'RF', 'SF', 'CumAccRF', 'CumAccSF']

    output = pd.Panel(items=dt_range, major_axis=cols, minor_axis=dframe.columns)
    forwards = dframe.shift(-1)

    # running the backtest at weekly intervals for specified date range
    for count, wdate in enumerate(dt_range):
        # calling the classify function and getting the required results
        pDate, pPred, pProbs, pScore = classify(clf, dframe=dframe, exfeat=exfeat, todate=wdate)
        dfPreds = pd.DataFrame(pPred, index=dframe.columns)
        dfProbs = pd.DataFrame(pProbs, index=dframe.columns, columns=['P1', 'P2', 'P3', 'P4', 'P5'])

        # recording the outputs in the output panel
        output.loc[wdate].ix['Pred'] = dfPreds.values.flatten()
        output.loc[wdate].iloc[1:6] = dfProbs.values.reshape(5, len(dfProbs))
        output.loc[wdate].ix['Act'] = forwards.loc[wdate]

        prev_mean = dframe.ix[:wdate][:-1].mean()
        prev_std = dframe.ix[:wdate][:-1].std()

        for name in dframe.columns:
            output.loc[wdate][name].ix['PrevMean'] = prev_mean[name]
            output.loc[wdate][name].ix['PrevSD'] = prev_std[name]
            output.loc[wdate][name].ix['ActLabel'] = cLabel(output.loc[wdate][name].ix['Act'],
                                                            prev_mean[name], prev_std[name])
            if output.loc[wdate][name].ix['ActLabel']==\
                        output.loc[wdate][name].ix['Pred']:
                output.loc[wdate][name].ix['RF'] = 1
            else:
                output.loc[wdate][name].ix['RF'] = 0
            if np.sign(output.loc[wdate][name].ix['Pred'])==\
                np.sign(output.loc[wdate][name].ix['ActLabel']):
                output.loc[wdate][name].ix['SF'] = 1
            else:
                output.loc[wdate][name].ix['SF'] = 0
        output.loc[wdate].ix['CumAccRF'] = (output.loc[:wdate].ix[:, 'RF'].sum(axis=1) /
                                            output.loc[:wdate].ix[:, 'RF'].count(axis=1))
        output.loc[wdate].ix['CumAccSF'] = (output.loc[:wdate].ix[:, 'SF'].sum(axis=1) /
                                            output.loc[:wdate].ix[:, 'SF'].count(axis=1))
    return output
